"""
Auto-dÃ©couverte des sections - Version 2
BasÃ©e sur le texte des liens, pas les URLs
"""
import requests
from bs4 import BeautifulSoup
import sqlite3
from urllib.parse import urljoin

def discover_sections():
    print("\n" + "="*70)
    print("ğŸ” AUTO-DÃ‰COUVERTE DES SECTIONS V2")
    print("="*70 + "\n")
    
    url = 'https://coursupreme.dz/'
    
    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=30)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Mots-clÃ©s pour identifier les sections de dÃ©cisions
    keywords = ['Ù‚Ø±Ø§Ø±Ø§Øª', 'Ø§Ù„ØºØ±Ù', 'Ù„Ø¬Ù†Ø©']
    
    # Exclure certains liens non pertinents
    exclude_keywords = ['ØªØ¹Ù„Ù†', 'Ø¥Ø³ØªØ´Ø§Ø±Ø©', 'Ø¥Ø¹Ù„Ø§Ù†']
    
    sections_found = []
    all_links = soup.find_all('a', href=True)
    
    for link in all_links:
        text = link.get_text(strip=True)
        href = link.get('href')
        
        # Doit contenir un mot-clÃ©
        if not any(keyword in text for keyword in keywords):
            continue
        
        # Ne doit pas contenir de mots exclus
        if any(exclude in text for exclude in exclude_keywords):
            continue
        
        # Ignorer le lien parent "Ù…Ù† Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§"
        if href == '#':
            continue
        
        # Ignorer les URLs relatives incomplÃ¨tes
        if href.startswith('/') and len(href) < 10:
            continue
        
        # Construire l'URL complÃ¨te
        full_url = urljoin('https://coursupreme.dz', href).rstrip('/')
        
        # VÃ©rifier que c'est une URL de section (pas une dÃ©cision individuelle)
        if '/decision/' in full_url:
            continue
        
        sections_found.append({
            'name_ar': text,
            'url': full_url
        })
    
    # DÃ©dupliquer par URL
    sections_unique = {}
    for section in sections_found:
        url_clean = section['url']
        if url_clean not in sections_unique:
            sections_unique[url_clean] = section
    
    sections_list = list(sections_unique.values())
    
    print(f"âœ… {len(sections_list)} sections dÃ©couvertes\n")
    
    for i, section in enumerate(sections_list, 1):
        print(f"{i}. {section['name_ar']}")
        print(f"   {section['url']}\n")
    
    return sections_list

def sync_to_db(sections):
    print("="*70)
    print("ğŸ’¾ SYNCHRONISATION BASE DE DONNÃ‰ES")
    print("="*70 + "\n")
    
    conn = sqlite3.connect('../../harvester.db')
    cursor = conn.cursor()
    
    # RÃ©cupÃ©rer existantes
    cursor.execute("SELECT url FROM supreme_court_chambers")
    existing_urls = {row[0] for row in cursor.fetchall()}
    
    added = 0
    
    for section in sections:
        if section['url'] not in existing_urls:
            cursor.execute("""
                INSERT INTO supreme_court_chambers (name_ar, name_fr, url, active)
                VALUES (?, ?, ?, 1)
            """, (section['name_ar'], section['name_ar'], section['url']))
            added += 1
            print(f"â• {section['name_ar']}")
    
    conn.commit()
    
    cursor.execute("SELECT COUNT(*) FROM supreme_court_chambers WHERE active = 1")
    total = cursor.fetchone()[0]
    
    conn.close()
    
    print(f"\nâœ… Nouvelles : {added}")
    print(f"âœ… Total BDD : {total}\n")
    
    return total

if __name__ == '__main__':
    sections = discover_sections()
    
    if sections:
        sync_to_db(sections)
