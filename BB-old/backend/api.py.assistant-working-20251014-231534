"""
API Flask pour le Moissonneur de Documents
Backend avec persistance SQLite + T√©l√©chargement automatique des fichiers
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import json
from datetime import datetime
import threading
import uuid
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, unquote
from analysis import get_api_key
from abc import ABC, abstractmethod
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import sqlite3
from contextlib import contextmanager
import os
import re

try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False


# Configuration
DB_PATH = 'harvester.db'
STORAGE_BASE_PATH = '/Users/djamel/Documents/textes_juridiques_DZ'


# ============================================================================
# FONCTIONS DE GESTION DES FICHIERS
# ============================================================================

def sanitize_filename(filename):
    """
    Transforme un nom de fichier ou URL en nom de fichier s√ªr.
    Remplace les caract√®res probl√©matiques par des underscores.
    """
    # Enlever le protocole si pr√©sent
    filename = re.sub(r'^https?://', '', filename)
    
    # Remplacer les caract√®res non autoris√©s par _
    safe_name = re.sub(r'[<>:"/\\|?*]', '_', filename)
    
    # Limiter la longueur (max 200 caract√®res + extension)
    if len(safe_name) > 200:
        # Garder l'extension
        name_parts = safe_name.rsplit('.', 1)
        if len(name_parts) == 2:
            base, ext = name_parts
            safe_name = base[:200] + '.' + ext
        else:
            safe_name = safe_name[:200]
    
    return safe_name


def get_site_name_from_url(url):
    """Extrait le hostname d'une URL pour l'utiliser comme nom de site"""
    try:
        parsed = urlparse(url)
        hostname = parsed.hostname or parsed.netloc
        # Enlever www. si pr√©sent
        return hostname.replace('www.', '') if hostname else 'unknown_site'
    except:
        return 'unknown_site'


def get_storage_path(base_url, site_name=None, subfolder=None):
    """
    Retourne le chemin complet du r√©pertoire de stockage.
    
    Args:
        base_url: URL de base du site
        site_name: Nom personnalis√© du site (optionnel)
        subfolder: Sous-dossier (ann√©e ou autre) (optionnel)
    
    Returns:
        Chemin complet du r√©pertoire
    """
    # D√©terminer le nom du site
    if site_name:
        site_folder = sanitize_filename(site_name)
    else:
        site_folder = get_site_name_from_url(base_url)
    
    # Construire le chemin
    path = os.path.join(STORAGE_BASE_PATH, site_folder)
    
    # Ajouter le sous-dossier si sp√©cifi√©
    if subfolder:
        subfolder_clean = sanitize_filename(str(subfolder))
        path = os.path.join(path, subfolder_clean)
    
    return path


def download_document(url, storage_path, timeout=60):
    """
    T√©l√©charge un document depuis une URL et le sauvegarde.
    
    Args:
        url: URL du document
        storage_path: Chemin complet du r√©pertoire de destination
        timeout: Timeout en secondes
    
    Returns:
        tuple: (success: bool, local_path: str or None, error: str or None)
    """
    try:
        # Cr√©er le r√©pertoire si n√©cessaire
        os.makedirs(storage_path, exist_ok=True)
        
        # G√©n√©rer le nom du fichier √† partir de l'URL
        parsed_url = urlparse(url)
        original_filename = unquote(parsed_url.path.split('/')[-1])
        
        # Si pas de nom de fichier dans l'URL, en cr√©er un
        if not original_filename or '.' not in original_filename:
            # Utiliser toute l'URL comme nom
            original_filename = sanitize_filename(url)
            # Ajouter .pdf par d√©faut si pas d'extension
            if '.' not in original_filename:
                original_filename += '.pdf'
        else:
            original_filename = sanitize_filename(original_filename)
        
        # Chemin complet du fichier
        local_path = os.path.join(storage_path, original_filename)
        
        # V√©rifier si le fichier existe d√©j√†
        if os.path.exists(local_path):
            print(f"   ‚è≠Ô∏è  Fichier existe d√©j√† : {original_filename}")
            return True, local_path, None
        
        # T√©l√©charger le fichier
        print(f"   üì• T√©l√©chargement : {original_filename}...")
        response = requests.get(url, timeout=timeout, stream=True)
        response.raise_for_status()
        
        # Sauvegarder le fichier
        with open(local_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        
        file_size = os.path.getsize(local_path)
        print(f"   ‚úÖ T√©l√©charg√© : {original_filename} ({file_size} bytes)")
        
        return True, local_path, None
        
    except requests.exceptions.Timeout:
        error = f"Timeout lors du t√©l√©chargement de {url}"
        print(f"   ‚è±Ô∏è  {error}")
        return False, None, error
        
    except requests.exceptions.RequestException as e:
        error = f"Erreur HTTP : {str(e)}"
        print(f"   ‚ùå {error}")
        return False, None, error
        
    except Exception as e:
        error = f"Erreur : {str(e)}"
        print(f"   ‚ùå {error}")
        return False, None, error


def delete_local_file(local_path):
    """
    Supprime un fichier local.
    
    Args:
        local_path: Chemin complet du fichier
    
    Returns:
        bool: True si succ√®s, False sinon
    """
    if not local_path or not os.path.exists(local_path):
        return True
    
    try:
        # Supprimer le fichier principal
        os.remove(local_path)
        print(f"   üóëÔ∏è  Fichier supprim√© : {local_path}")
        
        # Supprimer aussi le fichier .txt associ√© (texte extrait)
        txt_path = os.path.splitext(local_path)[0] + '.txt'
        if os.path.exists(txt_path):
            os.remove(txt_path)
            print(f"   üóëÔ∏è  Fichier texte supprim√© : {txt_path}")
        
        # Supprimer le r√©pertoire parent s'il est vide
        parent_dir = os.path.dirname(local_path)
        if os.path.exists(parent_dir) and not os.listdir(parent_dir):
            os.rmdir(parent_dir)
            print(f"   üóëÔ∏è  R√©pertoire vide supprim√© : {parent_dir}")
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå Erreur suppression fichier : {e}")
        return False


# ============================================================================
# FONCTIONS DE BASE DE DONN√âES
# ============================================================================

@contextmanager
def get_db_connection():
    """Context manager pour les connexions √† la base de donn√©es"""
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    try:
        yield conn
        conn.commit()
    except Exception as e:
        conn.rollback()
        raise e
    finally:
        conn.close()


def get_or_create_site(base_url):
    """R√©cup√®re ou cr√©e un site dans la BD"""
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        cursor.execute("SELECT id FROM sites WHERE base_url = ?", (base_url,))
        site = cursor.fetchone()
        
        if site:
            return site['id']
        
        now = datetime.now().isoformat()
        cursor.execute("""
            INSERT INTO sites (base_url, created_at, updated_at, total_documents)
            VALUES (?, ?, ?, 0)
        """, (base_url, now, now))
        
        return cursor.lastrowid


def save_harvest_parameters(site_id, params, scope='session'):
    """Sauvegarde les param√®tres de moissonnage"""
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        now = datetime.now().isoformat()
        data_json = json.dumps(params)
        
        cursor.execute("""
            INSERT INTO harvest_parameters (site_id, created_at, data, scope)
            VALUES (?, ?, ?, ?)
        """, (site_id, now, data_json, scope))
        
        return cursor.lastrowid


def create_harvest_session(site_id, job_uuid, parameters_id=None):
    """Cr√©e une nouvelle session de moissonnage"""
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        now = datetime.now().isoformat()
        
        cursor.execute("""
            INSERT INTO harvest_sessions 
            (job_uuid, site_id, parameters_id, started_at, status, 
             total_found, duplicates_removed, new_documents, total_documents)
            VALUES (?, ?, ?, ?, 'running', 0, 0, 0, 0)
        """, (job_uuid, site_id, parameters_id, now))
        
        return cursor.lastrowid


def update_harvest_session(session_id, status, total_found=None, error_message=None):
    """Met √† jour une session de moissonnage"""
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        now = datetime.now().isoformat()
        
        if status == 'completed':
            cursor.execute("""
                UPDATE harvest_sessions
                SET status = ?, completed_at = ?, total_found = ?
                WHERE id = ?
            """, (status, now, total_found or 0, session_id))
        elif status == 'error':
            cursor.execute("""
                UPDATE harvest_sessions
                SET status = ?, completed_at = ?, error_message = ?
                WHERE id = ?
            """, (status, now, error_message, session_id))
        else:
            cursor.execute("""
                UPDATE harvest_sessions
                SET status = ?
                WHERE id = ?
            """, (status, session_id))


def insert_document(site_id, session_id, doc_data):
    """
    Ins√®re un document dans la BD.
    Retourne True si ins√©r√©, False si doublon.
    """
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        now = datetime.now().isoformat()
        
        # Pr√©parer extra_metadata
        extra_metadata = {}
        if 'number' in doc_data:
            extra_metadata['number'] = doc_data['number']
        if 'year' in doc_data:
            extra_metadata['year'] = doc_data['year']
        
        extra_metadata_json = json.dumps(extra_metadata) if extra_metadata else None
        
        try:
            cursor.execute("""
                INSERT INTO documents 
                (site_id, session_id, url, filename, title, file_type, 
                 file_size, last_modified, accessible, added_at, extra_metadata,
                 local_path, downloaded)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                site_id,
                session_id,
                doc_data['url'],
                doc_data.get('filename'),
                doc_data.get('title'),
                doc_data.get('file_type'),
                doc_data.get('file_size'),
                doc_data.get('last_modified'),
                doc_data.get('accessible', True),
                now,
                extra_metadata_json,
                None,  # local_path - sera mis √† jour apr√®s t√©l√©chargement
                0      # downloaded - False par d√©faut
            ))
            
            return True, cursor.lastrowid
            
        except sqlite3.IntegrityError:
            # Doublon d√©tect√©
            return False, None


def update_document_local_path(doc_id, local_path):
    """Met √† jour le chemin local d'un document"""
    with get_db_connection() as conn:
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE documents
            SET local_path = ?, downloaded = 1
            WHERE id = ?
        """, (local_path, doc_id))


def update_session_statistics(session_id):
    """Met √† jour les statistiques de la session"""
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT COUNT(*) as count
            FROM documents
            WHERE session_id = ?
        """, (session_id,))
        
        new_docs = cursor.fetchone()['count']
        
        cursor.execute("""
            UPDATE harvest_sessions
            SET new_documents = ?,
                total_documents = (SELECT COUNT(*) FROM documents WHERE site_id = 
                    (SELECT site_id FROM harvest_sessions WHERE id = ?))
            WHERE id = ?
        """, (new_docs, session_id, session_id))
        
        return new_docs


def delete_empty_session(session_id):
    """Supprime une session si elle n'a aucun nouveau document"""
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT new_documents FROM harvest_sessions WHERE id = ?
        """, (session_id,))
        
        session = cursor.fetchone()
        if session and session['new_documents'] == 0:
            cursor.execute("DELETE FROM harvest_sessions WHERE id = ?", (session_id,))
            return True
        
        return False


def get_session_by_uuid(job_uuid):
    """R√©cup√®re une session par son UUID"""
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT hs.*, s.base_url
            FROM harvest_sessions hs
            JOIN sites s ON hs.site_id = s.id
            WHERE hs.job_uuid = ?
        """, (job_uuid,))
        
        return cursor.fetchone()


def get_session_documents(session_id):
    """R√©cup√®re tous les documents d'une session"""
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT * FROM documents
            WHERE session_id = ?
            ORDER BY added_at
        """, (session_id,))
        
        docs = cursor.fetchall()
        
        result = []
        for doc in docs:
            doc_dict = dict(doc)
            if doc_dict['extra_metadata']:
                extra = json.loads(doc_dict['extra_metadata'])
                doc_dict.update(extra)
            del doc_dict['extra_metadata']
            result.append(doc_dict)
        
        return result


def get_all_sessions():
    """R√©cup√®re toutes les sessions de l'historique"""
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT 
                hs.job_uuid as id,
                hs.status,
                hs.new_documents as document_count,
                hs.started_at,
                hs.completed_at,
                s.base_url
            FROM harvest_sessions hs
            JOIN sites s ON hs.site_id = s.id
            ORDER BY hs.started_at DESC
        """)
        
        sessions = cursor.fetchall()
        return [dict(session) for session in sessions]


# ============================================================================
# CLASSES DE MOISSONNAGE
# ============================================================================

class BaseHarvester(ABC):
    DOCUMENT_EXTENSIONS = {
        '.pdf', '.docx', '.doc', '.xlsx', '.xls',
        '.pptx', '.ppt', '.png', '.jpg', '.jpeg',
        '.gif', '.txt', '.csv', '.zip'
    }
    
    def __init__(self, base_url, profile=None):
        self.base_url = base_url
        self.profile = profile or {}
        self.documents = []
    
    @abstractmethod
    def harvest(self, max_results=10):
        pass
    
    def format_file_size(self, size_bytes):
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} TB"
    
    def get_document_info(self, url):
        try:
            response = requests.head(url, timeout=60, allow_redirects=True)
            info = {}
            
            if response.status_code == 200:
                if 'content-length' in response.headers:
                    size_bytes = int(response.headers['content-length'])
                    info['file_size'] = self.format_file_size(size_bytes)
                
                if 'last-modified' in response.headers:
                    info['last_modified'] = response.headers['last-modified']
                
                path = urlparse(url).path
                filename = unquote(path.split('/')[-1])
                if filename:
                    info['filename'] = filename
                
                info['accessible'] = True
            else:
                info['accessible'] = False
            
            return info
        except:
            return {'accessible': False}


class GenericHarvester(BaseHarvester):
    def matches_filters(self, metadata):
        profile = self.profile
        
        if profile.get('extensions'):
            ext = f".{metadata.get('file_type', '')}"
            if ext not in profile['extensions']:
                return False
        
        if profile.get('keywords'):
            title = metadata.get('title', '').lower()
            keywords = [k.strip().lower() for k in profile['keywords'].split(',')]
            if not any(kw in title for kw in keywords if kw):
                return False
        
        if profile.get('minSize') or profile.get('maxSize'):
            size_str = metadata.get('file_size', '')
            if size_str:
                try:
                    if 'KB' in size_str:
                        size_kb = float(size_str.split()[0])
                    elif 'MB' in size_str:
                        size_kb = float(size_str.split()[0]) * 1024
                    elif 'GB' in size_str:
                        size_kb = float(size_str.split()[0]) * 1024 * 1024
                    else:
                        size_kb = 0
                    
                    if profile.get('minSize') and size_kb < float(profile['minSize']):
                        return False
                    if profile.get('maxSize') and size_kb > float(profile['maxSize']):
                        return False
                except:
                    pass
        
        return True
    
    def harvest(self, max_results=10):
        try:
            response = requests.get(self.base_url, timeout=60)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            links = soup.find_all('a', href=True)
            
            for link in links:
                if len(self.documents) >= max_results:
                    break
                
                href = link['href']
                full_url = urljoin(self.base_url, href)
                path_lower = urlparse(full_url).path.lower()
                
                is_document = any(path_lower.endswith(ext) for ext in self.DOCUMENT_EXTENSIONS)
                
                if is_document:
                    metadata = {
                        'url': full_url,
                        'title': link.get('title') or link.get_text(strip=True),
                        'file_type': path_lower.split('.')[-1]
                    }
                    
                    path = urlparse(full_url).path
                    filename = unquote(path.split('/')[-1])
                    if filename:
                        metadata['filename'] = filename
                    
                    doc_info = self.get_document_info(full_url)
                    metadata.update(doc_info)
                    
                    if self.matches_filters(metadata):
                        self.documents.append(metadata)
            
            return self.documents
        except Exception as e:
            raise Exception(f"Erreur lors du moissonnage: {str(e)}")


class JORADPHarvester(BaseHarvester):
    def __init__(self, base_url, profile=None, year=2025, config=None):
        super().__init__(base_url, profile)
        self.year = year
        self.config = config or {
            'workers': 2,
            'timeout': 60,
            'retry_count': 3,
            'delay_between': 0.5
        }
    
    def build_pdf_url(self, number):
        padded_number = str(number).zfill(3)
        return f"https://www.joradp.dz/FTP/JO-FRANCAIS/{self.year}/F{self.year}{padded_number}.pdf"
    
    def check_pdf_exists(self, number):
        url = self.build_pdf_url(number)
        
        max_retries = self.config.get('retry_count', 3)
        timeout = self.config.get('timeout', 60)
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                response = requests.head(url, timeout=timeout, allow_redirects=True)
                if response.status_code == 200:
                    metadata = {
                        'url': url,
                        'number': str(number).zfill(3),
                        'title': f"Journal Officiel N¬∞{str(number).zfill(3)} - {self.year}",
                        'filename': f"F{self.year}{str(number).zfill(3)}.pdf",
                        'year': self.year,
                        'file_type': 'pdf',
                        'accessible': True
                    }
                    
                    if 'content-length' in response.headers:
                        size_bytes = int(response.headers['content-length'])
                        metadata['file_size'] = self.format_file_size(size_bytes)
                    
                    if 'last-modified' in response.headers:
                        metadata['last_modified'] = response.headers['last-modified']
                    
                    return metadata
                else:
                    if response.status_code == 404:
                        return None
                    if attempt < max_retries - 1:
                        time.sleep(retry_delay)
                        continue
                    return None
                    
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    continue
                return None
            except Exception as e:
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    continue
                return None
        
        return None
    
    def harvest(self, max_results=10, start=1, end=100):
        found_count = 0
        workers = self.config.get('workers', 2)
        delay = self.config.get('delay_between', 0.5)
        
        if workers == 1:
            print(f"üîÑ Mode s√©quentiel activ√© pour N¬∞{start} √† {end}")
            for num in range(start, end + 1):
                if max_results and found_count >= max_results:
                    break
                
                result = self.check_pdf_exists(num)
                if result:
                    found_count += 1
                    self.documents.append(result)
                    print(f"‚úì Trouv√©: N¬∞{result['number']}")
                else:
                    print(f"‚úó Absent: N¬∞{str(num).zfill(3)}")
                
                time.sleep(delay)
        else:
            print(f"‚ö° Mode parall√®le activ√© ({workers} workers)")
            with ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_number = {
                    executor.submit(self.check_pdf_exists, num): num
                    for num in range(start, end + 1)
                }
                
                for future in as_completed(future_to_number):
                    result = future.result()
                    if result:
                        found_count += 1
                        self.documents.append(result)
                        
                        if max_results and found_count >= max_results:
                            for f in future_to_number:
                                f.cancel()
                            break
                    
                    time.sleep(delay)
        
        self.documents.sort(key=lambda x: x['number'])
        return self.documents


# ============================================================================
# API FLASK
# ============================================================================

from analysis_routes import register_analysis_routes
app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False  # Supporter les caract√®res fran√ßais
register_analysis_routes(app)
CORS(app)

active_jobs = {}


@app.route('/api/harvesters', methods=['GET'])
def get_harvesters():
    return jsonify({
        'harvesters': [
            {
                'id': 'generic',
                'name': 'Moissonneur G√©n√©rique',
                'description': 'Pour sites HTML standard avec liens directs',
                'params': ['url', 'max_results', 'extensions']
            },
            {
                'id': 'joradp',
                'name': 'JORADP (Journal Officiel Alg√©rie)',
                'description': 'Moissonneur sp√©cialis√© pour le JORADP',
                'params': ['year', 'start', 'end', 'max_results']
            },
            {
                'id': 'selenium',
                'name': 'Moissonneur JavaScript',
                'description': 'Pour sites avec JavaScript (n√©cessite Selenium)',
                'params': ['url', 'max_results', 'wait_time'],
                'available': SELENIUM_AVAILABLE
            }
        ]
    })


@app.route('/api/harvest', methods=['POST'])
def start_harvest():
    data = request.json
    
    harvester_type = data.get('harvester_type', 'generic')
    url = data.get('url')
    
    if not url and harvester_type != 'joradp':
        return jsonify({'error': 'URL requise'}), 400
    
    # D√©terminer l'URL de base
    if harvester_type == 'joradp':
        base_url = "https://www.joradp.dz/HFR/Index.htm"
    else:
        base_url = url
    
    # Cr√©er ou r√©cup√©rer le site
    site_id = get_or_create_site(base_url)
    
    # Sauvegarder les param√®tres
    parameters_id = save_harvest_parameters(site_id, data, scope='session')
    
    # Cr√©er le job
    job_uuid = str(uuid.uuid4())
    session_id = create_harvest_session(site_id, job_uuid, parameters_id)
    
    # Stocker temporairement
    active_jobs[job_uuid] = {
        'session_id': session_id,
        'site_id': site_id,
        'status': 'running',
        'progress': 0
    }
    
    # Lancer le moissonnage dans un thread
    thread = threading.Thread(
        target=run_harvest,
        args=(job_uuid, session_id, site_id, harvester_type, data)
    )
    thread.start()
    
    return jsonify({
        'job_id': job_uuid,
        'status': 'started'
    })


def run_harvest(job_uuid, session_id, site_id, harvester_type, params):
    try:
        # Cr√©er le harvester
        if harvester_type == 'joradp':
            year = int(params.get('year', 2025))
            start = int(params.get('start', 1))
            end = int(params.get('end', 100))
            max_results = int(params.get('max_results', 10))
            
            config = {
                'workers': int(params.get('workers', 2)),
                'timeout': int(params.get('timeout', 60)),
                'retry_count': int(params.get('retry_count', 3)),
                'delay_between': float(params.get('delay_between', 0.5))
            }
            
            harvester = JORADPHarvester(
                base_url="https://www.joradp.dz/HFR/Index.htm",
                year=year,
                config=config
            )
            documents = harvester.harvest(max_results=max_results, start=start, end=end)
        
        elif harvester_type == 'generic':
            url = params.get('url')
            max_results = int(params.get('max_results', 10))
            
            profile = {}
            if params.get('extensions'):
                profile['extensions'] = params['extensions']
            if params.get('keywords'):
                profile['keywords'] = params['keywords']
            if params.get('minSize'):
                profile['minSize'] = params['minSize']
            if params.get('maxSize'):
                profile['maxSize'] = params['maxSize']
            
            harvester = GenericHarvester(url, profile=profile)
            documents = harvester.harvest(max_results=max_results)
        
        else:
            raise ValueError(f"Type de moissonneur non support√©: {harvester_type}")
        
        # Ins√©rer les documents dans la BD
        duplicates_count = 0
        inserted_docs = []
        
        for doc in documents:
            inserted, doc_id = insert_document(site_id, session_id, doc)
            if inserted:
                inserted_docs.append((doc_id, doc))
            else:
                duplicates_count += 1
        
        print(f"\nüìä Statistiques d'insertion:")
        print(f"   ‚Ä¢ Documents trouv√©s : {len(documents)}")
        print(f"   ‚Ä¢ Nouveaux documents : {len(inserted_docs)}")
        print(f"   ‚Ä¢ Doublons ignor√©s : {duplicates_count}")
        
        # T√©l√©charger les fichiers pour les nouveaux documents
        if inserted_docs:
            print(f"\nüì• T√©l√©chargement des fichiers...")
            
            # D√©terminer le chemin de stockage
            base_url = harvester.base_url
            site_name = params.get('site_name')
            subfolder = params.get('subfolder') or params.get('year')
            
            storage_path = get_storage_path(base_url, site_name, subfolder)
            print(f"   üìÅ R√©pertoire de stockage : {storage_path}")
            
            # T√©l√©charger chaque document
            download_errors = 0
            for doc_id, doc in inserted_docs:
                success, local_path, error = download_document(
                    doc['url'], 
                    storage_path,
                    timeout=int(params.get('timeout', 60))
                )
                
                if success:
                    # Mettre √† jour le chemin local dans la BD
                    update_document_local_path(doc_id, local_path)
                else:
                    download_errors += 1
                    print(f"   ‚ö†Ô∏è  √âchec t√©l√©chargement : {doc['url']}")
            
            print(f"\n‚úÖ T√©l√©chargement termin√© :")
            print(f"   ‚Ä¢ Fichiers t√©l√©charg√©s : {len(inserted_docs) - download_errors}")
            print(f"   ‚Ä¢ Erreurs : {download_errors}")
        
        # Mettre √† jour les statistiques
        new_docs_count = update_session_statistics(session_id)
        
        # Mettre √† jour le compteur de doublons
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE harvest_sessions
                SET duplicates_removed = ?
                WHERE id = ?
            """, (duplicates_count, session_id))
        
        # Marquer comme termin√©
        update_harvest_session(session_id, 'completed', len(documents))
        
        # Supprimer la session si vide
        was_deleted = delete_empty_session(session_id)
        
        # Mettre √† jour le job actif
        if job_uuid in active_jobs:
            active_jobs[job_uuid]['status'] = 'deleted' if was_deleted else 'completed'
            active_jobs[job_uuid]['progress'] = 100
            active_jobs[job_uuid]['new_documents'] = new_docs_count
            active_jobs[job_uuid]['duplicates'] = duplicates_count
        
        print(f"\nüéâ Moissonnage termin√© : {new_docs_count} nouveaux docs, {duplicates_count} doublons")
        
    except Exception as e:
        print(f"\n‚ùå Erreur : {str(e)}")
        update_harvest_session(session_id, 'error', error_message=str(e))
        
        if job_uuid in active_jobs:
            active_jobs[job_uuid]['status'] = 'error'
            active_jobs[job_uuid]['error'] = str(e)


@app.route('/api/harvest/<job_id>', methods=['GET'])
def get_harvest_status(job_id):
    if job_id in active_jobs:
        job = active_jobs[job_id]
        
        if job['status'] in ['completed', 'deleted']:
            session = get_session_by_uuid(job_id)
            if session:
                documents = get_session_documents(session['id'])
                
                return jsonify({
                    'id': job_id,
                    'status': job['status'],
                    'documents': documents,
                    'document_count': len(documents),
                    'started_at': session['started_at'],
                    'completed_at': session['completed_at']
                })
            else:
                return jsonify({
                    'id': job_id,
                    'status': 'deleted',
                    'message': 'Session supprim√©e (tous les documents √©taient des doublons)',
                    'documents': [],
                    'document_count': 0
                })
        
        return jsonify({
            'id': job_id,
            'status': job['status'],
            'progress': job['progress']
        })
    
    session = get_session_by_uuid(job_id)
    if not session:
        return jsonify({'error': 'Job introuvable'}), 404
    
    return jsonify({
        'id': job_id,
        'status': session['status'],
        'started_at': session['started_at'],
        'completed_at': session['completed_at']
    })


@app.route('/api/harvest/<job_id>/results', methods=['GET'])
def get_harvest_results(job_id):
    session = get_session_by_uuid(job_id)
    
    if not session:
        return jsonify({'error': 'Job introuvable'}), 404
    
    if session['status'] != 'completed':
        return jsonify({'error': 'Moissonnage non termin√©'}), 400
    
    documents = get_session_documents(session['id'])
    
    return jsonify({
        'job_id': job_id,
        'document_count': len(documents),
        'documents': documents,
        'started_at': session['started_at'],
        'completed_at': session['completed_at']
    })


@app.route('/api/harvest/<job_id>/export', methods=['GET'])
def export_harvest_results(job_id):
    session = get_session_by_uuid(job_id)
    
    if not session:
        return jsonify({'error': 'Job introuvable'}), 404
    
    # R√©cup√©rer le param√®tre include_analysis
    include_analysis = request.args.get('include_analysis', 'false').lower() == 'true'
    
    documents = get_session_documents(session['id'])
    
    # Enrichir avec les analyses si demand√©
    if include_analysis:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            for doc in documents:
                cursor.execute("""
                    SELECT analyzed, analysis_status, ai_analysis, full_text_path
                    FROM documents 
                    WHERE id = ?
                """, (doc['id'],))
                
                analysis_data = cursor.fetchone()
                if analysis_data:
                    doc['analysis'] = {
                        'analyzed': bool(analysis_data[0]),
                        'status': analysis_data[1],
                        'full_text_available': bool(analysis_data[3])
                    }
                    
                    # Parser le JSON ai_analysis s'il existe
                    if analysis_data[2]:
                        try:
                            doc['analysis']['result'] = json.loads(analysis_data[2])
                        except json.JSONDecodeError:
                            doc['analysis']['result'] = None
    
    result = {
        'job_id': job_id,
        'export_date': datetime.now().isoformat(),
        'document_count': len(documents),
        'documents': documents,
        'include_analysis': include_analysis,
        'statistics': {
            'total_found': session['total_found'],
            'duplicates_removed': session['duplicates_removed'],
            'new_documents': session['new_documents']
        }
    }
    
    # Forcer UTF-8 dans la r√©ponse JSON
    response = app.response_class(
        response=json.dumps(result, ensure_ascii=False, indent=2),
        status=200,
        mimetype='application/json; charset=utf-8'
    )
    return response


@app.route('/api/jobs', methods=['GET'])
def get_all_jobs():
    sessions = get_all_sessions()
    return jsonify({'jobs': sessions})


@app.route('/api/jobs/<job_id>', methods=['DELETE'])
def delete_job(job_id):
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        cursor.execute("SELECT id FROM harvest_sessions WHERE job_uuid = ?", (job_id,))
        session = cursor.fetchone()
        
        if not session:
            return jsonify({'error': 'Job introuvable'}), 404
        
        session_id = session['id']
        
        # R√©cup√©rer les chemins locaux avant suppression
        cursor.execute("SELECT local_path FROM documents WHERE session_id = ?", (session_id,))
        docs = cursor.fetchall()
        
        # Supprimer les fichiers physiques
        for doc in docs:
            if doc['local_path']:
                delete_local_file(doc['local_path'])
        
        # Supprimer les documents de la BD
        cursor.execute("DELETE FROM documents WHERE session_id = ?", (session_id,))
        
        # Supprimer la session
        cursor.execute("DELETE FROM harvest_sessions WHERE id = ?", (session_id,))
    
    return jsonify({'success': True, 'message': 'Job et fichiers supprim√©s d√©finitivement'})


@app.route('/api/jobs/bulk-delete', methods=['POST'])
def bulk_delete_jobs():
    job_ids = request.json.get('job_ids', [])
    
    deleted_count = 0
    
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        for job_id in job_ids:
            cursor.execute("SELECT id FROM harvest_sessions WHERE job_uuid = ?", (job_id,))
            session = cursor.fetchone()
            
            if session:
                session_id = session['id']
                
                # Supprimer les fichiers physiques
                cursor.execute("SELECT local_path FROM documents WHERE session_id = ?", (session_id,))
                docs = cursor.fetchall()
                for doc in docs:
                    if doc['local_path']:
                        delete_local_file(doc['local_path'])
                
                # Supprimer de la BD
                cursor.execute("DELETE FROM documents WHERE session_id = ?", (session_id,))
                cursor.execute("DELETE FROM harvest_sessions WHERE id = ?", (session_id,))
                deleted_count += 1
    
    return jsonify({
        'success': True,
        'deleted_count': deleted_count,
        'message': f'{deleted_count} job(s) et fichiers supprim√©s d√©finitivement'
    })


@app.route('/api/duplicates', methods=['POST'])
def detect_duplicates():
    job_ids = request.json.get('job_ids', [])
    
    if not job_ids:
        return jsonify({'duplicates': []})
    
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        placeholders = ','.join('?' * len(job_ids))
        cursor.execute(f"""
            SELECT d.*, hs.job_uuid, hs.started_at as session_date
            FROM documents d
            JOIN harvest_sessions hs ON d.session_id = hs.id
            WHERE hs.job_uuid IN ({placeholders})
            ORDER BY d.added_at
        """, job_ids)
        
        all_docs = cursor.fetchall()
        
        url_map = {}
        duplicates = []
        
        for doc in all_docs:
            doc_dict = dict(doc)
            url = doc_dict['url']
            
            if url in url_map:
                original = url_map[url]
                duplicates.append({
                    'url': url,
                    'original': {
                        'job_id': original['job_uuid'],
                        'doc_index': 0,
                        'title': original['title'],
                        'filename': original['filename'],
                        'created_at': original['session_date']
                    },
                    'duplicate': {
                        'job_id': doc_dict['job_uuid'],
                        'doc_index': 0,
                        'title': doc_dict['title'],
                        'filename': doc_dict['filename'],
                        'created_at': doc_dict['session_date']
                    }
                })
            else:
                url_map[url] = doc_dict
    
    return jsonify({
        'duplicate_count': len(duplicates),
        'duplicates': duplicates
    })


@app.route('/api/harvest/<job_id>/document/<int:doc_index>/delete', methods=['POST'])
def delete_document(job_id, doc_index):
    session = get_session_by_uuid(job_id)
    
    if not session:
        return jsonify({'error': 'Job introuvable'}), 404
    
    documents = get_session_documents(session['id'])
    
    if doc_index < 0 or doc_index >= len(documents):
        return jsonify({'error': 'Index invalide'}), 400
    
    doc = documents[doc_index]
    doc_id = doc['id']
    local_path = doc.get('local_path')
    
    # Supprimer le fichier physique
    if local_path:
        delete_local_file(local_path)
    
    # Supprimer de la BD
    with get_db_connection() as conn:
        cursor = conn.cursor()
        cursor.execute("DELETE FROM documents WHERE id = ?", (doc_id,))
    
    # Mettre √† jour les statistiques
    new_docs_count = update_session_statistics(session['id'])
    
    # V√©rifier si la session est vide
    was_deleted = delete_empty_session(session['id'])
    
    if was_deleted:
        return jsonify({
            'success': True,
            'message': 'Document et fichier supprim√©s',
            'job_deleted': True,
            'info': 'Le moissonnage a √©t√© supprim√© car tous ses documents sont supprim√©s'
        })
    
    return jsonify({
        'success': True,
        'message': 'Document et fichier supprim√©s',
        'job_deleted': False,
        'remaining_documents': new_docs_count
    })


@app.route('/api/harvest/<job_id>/document/<int:doc_index>/rename', methods=['POST'])
def rename_document(job_id, doc_index):
    session = get_session_by_uuid(job_id)
    
    if not session:
        return jsonify({'error': 'Job introuvable'}), 404
    
    documents = get_session_documents(session['id'])
    
    if doc_index < 0 or doc_index >= len(documents):
        return jsonify({'error': 'Index invalide'}), 400
    
    doc_id = documents[doc_index]['id']
    new_filename = request.json.get('filename')
    
    if not new_filename:
        return jsonify({'error': 'Nom de fichier requis'}), 400
    
    with get_db_connection() as conn:
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE documents
            SET filename = ?
            WHERE id = ?
        """, (new_filename, doc_id))
    
    return jsonify({'success': True, 'message': 'Document renomm√©'})


@app.route('/api/document/<int:doc_id>/redownload', methods=['POST'])
def redownload_document(doc_id):
    """Re-t√©l√©charge un document si le fichier local est manquant"""
    
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        # R√©cup√©rer le document
        cursor.execute("""
            SELECT d.*, hs.parameters_id
            FROM documents d
            JOIN harvest_sessions hs ON d.session_id = hs.id
            WHERE d.id = ?
        """, (doc_id,))
        
        doc = cursor.fetchone()
        
        if not doc:
            return jsonify({'error': 'Document introuvable'}), 404
        
        # R√©cup√©rer les param√®tres pour le chemin de stockage
        cursor.execute("""
            SELECT data FROM harvest_parameters WHERE id = ?
        """, (doc['parameters_id'],))
        
        params_row = cursor.fetchone()
        params = json.loads(params_row['data']) if params_row else {}
        
        # D√©terminer le chemin de stockage
        cursor.execute("SELECT base_url FROM sites WHERE id = ?", (doc['site_id'],))
        site = cursor.fetchone()
        base_url = site['base_url']
        
        site_name = params.get('site_name')
        subfolder = params.get('subfolder') or params.get('year')
        
        storage_path = get_storage_path(base_url, site_name, subfolder)
        
        # T√©l√©charger
        success, local_path, error = download_document(
            doc['url'],
            storage_path,
            timeout=int(params.get('timeout', 60))
        )
        
        if success:
            # Mettre √† jour le chemin local
            cursor.execute("""
                UPDATE documents
                SET local_path = ?, downloaded = 1
                WHERE id = ?
            """, (local_path, doc_id))
            
            conn.commit()
            
            return jsonify({
                'success': True,
                'message': 'Document re-t√©l√©charg√© avec succ√®s',
                'local_path': local_path
            })
        else:
            return jsonify({
                'success': False,
                'error': error
            }), 500


@app.route('/api/health', methods=['GET'])
def health_check():
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM harvest_sessions")
            session_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM documents")
            doc_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM documents WHERE downloaded = 1")
            downloaded_count = cursor.fetchone()[0]
        
        # V√©rifier le r√©pertoire de stockage
        storage_exists = os.path.exists(STORAGE_BASE_PATH)
        storage_writable = os.access(STORAGE_BASE_PATH, os.W_OK) if storage_exists else False
        
        return jsonify({
            'status': 'healthy',
            'database': 'connected',
            'selenium_available': SELENIUM_AVAILABLE,
            'storage': {
                'base_path': STORAGE_BASE_PATH,
                'exists': storage_exists,
                'writable': storage_writable
            },
            'statistics': {
                'total_sessions': session_count,
                'total_documents': doc_count,
                'downloaded_files': downloaded_count
            },
            'timestamp': datetime.now().isoformat()
        })
    except Exception as e:
        return jsonify({
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500



# ==========================================
# ASSISTANT IA - Route pour le chat
# ==========================================

@app.route('/api/assistant/chat', methods=['POST'])
def assistant_chat():
    """Route pour dialoguer avec l'assistant de configuration"""
    import requests
    
    try:
        data = request.json
        messages = data.get('messages', [])
        system_prompt = data.get('system', '')
        
        # Appel √† l'API Claude via requests
        api_response = requests.post(
            'https://api.anthropic.com/v1/messages',
            headers={
                'Content-Type': 'application/json',
                'x-api-key': get_api_key(),
                'anthropic-version': '2023-06-01'
            },
            json={
                'model': 'claude-sonnet-4-20250514',
                'max_tokens': 2000,
                'messages': messages,
                'system': system_prompt
            },
            timeout=30
        )
        
        if api_response.status_code == 200:
            return jsonify(api_response.json())
        else:
            error_msg = f'API error: {api_response.status_code}'
            print(error_msg)
            return jsonify({'error': error_msg}), api_response.status_code
            
    except requests.exceptions.Timeout:
        print("Timeout lors de l'appel √† l'API Claude")
        return jsonify({'error': 'Request timeout'}), 408
    except Exception as e:
        print(f"Erreur assistant: {e}")
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    print("=" * 80)
    print("üöÄ API Moissonneur de Documents - Version avec T√©l√©chargement Automatique")
    print("=" * 80)
    print(f"\nüìä Base de donn√©es: {DB_PATH}")
    print(f"üìÅ Stockage des fichiers: {STORAGE_BASE_PATH}")
    
    # V√©rifier la connexion √† la BD
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM harvest_sessions")
            session_count = cursor.fetchone()[0]
            cursor.execute("SELECT COUNT(*) FROM documents")
            doc_count = cursor.fetchone()[0]
            cursor.execute("SELECT COUNT(*) FROM documents WHERE downloaded = 1")
            downloaded_count = cursor.fetchone()[0]
            cursor.execute("SELECT COUNT(*) FROM sites")
            site_count = cursor.fetchone()[0]
            
            print(f"\n‚úÖ Base de donn√©es connect√©e")
            print(f"   ‚Ä¢ {site_count} site(s)")
            print(f"   ‚Ä¢ {session_count} session(s) de moissonnage")
            print(f"   ‚Ä¢ {doc_count} document(s) ({downloaded_count} t√©l√©charg√©s)")
    except Exception as e:
        print(f"\n‚ùå Erreur de connexion √† la base de donn√©es: {e}")
    
    # V√©rifier le r√©pertoire de stockage
    if os.path.exists(STORAGE_BASE_PATH):
        print(f"\n‚úÖ R√©pertoire de stockage accessible")
        if os.access(STORAGE_BASE_PATH, os.W_OK):
            print(f"   ‚Ä¢ Permissions d'√©criture: OK")
        else:
            print(f"   ‚ö†Ô∏è  Permissions d'√©criture: MANQUANTES")
    else:
        print(f"\n‚ö†Ô∏è  R√©pertoire de stockage n'existe pas: {STORAGE_BASE_PATH}")
        print(f"   Le r√©pertoire sera cr√©√© automatiquement au premier t√©l√©chargement")
    
    print("\n‚ú® NOUVEAUT√âS:")
    print("   ‚Ä¢ T√©l√©chargement automatique des PDFs apr√®s moissonnage")
    print("   ‚Ä¢ Organisation par site et sous-dossier (ann√©e)")
    print("   ‚Ä¢ Suppression physique lors de la suppression d'un document")
    print("   ‚Ä¢ Endpoint de re-t√©l√©chargement si fichier manquant")
    print("   ‚Ä¢ Nom de fichier = URL (unicit√© garantie)")
    
    print("\nüì° Endpoints disponibles:")
    print("  GET  /api/harvesters          - Liste des moissonneurs")
    print("  POST /api/harvest             - Lancer un moissonnage + t√©l√©chargement")
    print("  GET  /api/harvest/<id>        - Statut d'un moissonnage")
    print("  GET  /api/harvest/<id>/results - R√©sultats")
    print("  GET  /api/harvest/<id>/export  - Exporter en JSON")
    print("  POST /api/harvest/<id>/document/<idx>/delete - Supprimer document + fichier")
    print("  POST /api/harvest/<id>/document/<idx>/rename - Renommer un document")
    print("  POST /api/document/<id>/redownload - Re-t√©l√©charger un fichier")
    print("  POST /api/duplicates          - D√©tecter les doublons")
    print("  GET  /api/jobs                - Liste tous les jobs")
    print("  DELETE /api/jobs/<id>         - Supprimer un job + fichiers")
    print("  POST /api/jobs/bulk-delete    - Supprimer plusieurs jobs + fichiers")
    print("  GET  /api/health              - Sant√© de l'API + stats stockage")
    
    print("\nüí° R√®gles de gestion:")
    print("   ‚Ä¢ Les doublons ne sont JAMAIS ins√©r√©s (UNIQUE sur site_id + url)")
    print("   ‚Ä¢ Les sessions sans nouveaux documents sont supprim√©es automatiquement")
    print("   ‚Ä¢ Les fichiers sont t√©l√©charg√©s APR√àS l'enregistrement en BD")
    print("   ‚Ä¢ Suppression d'un document = suppression du fichier physique")
    
    print("\n" + "=" * 80)
    print("\nüåê Serveur d√©marr√© sur http://localhost:5000")
    print("üí° Ouvrez l'application React sur http://localhost:3000\n")
    
    app.run(debug=True, port=5001)